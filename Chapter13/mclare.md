# Chapter 13

- Why is $I(p) = -\log_2 p$ negative?
- Funny that whenever the same formula crops up in another discipline, we choose to utilize the same term (entropy in this case), even if the implication of that meaning doesn't really apply.
- Entropy = average amount of information in the system, so why do we prove that its expected value is less than or equal to the average code length? Doesn't that follow intuitively? Or am I misunderstanding what is meant by expected value and average code length?

